{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 4295,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 3090
        },
        {
          "sourceId": 5111,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 3899
        },
        {
          "sourceId": 4690,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 3480
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "chatGLM2-text-generation",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mudogruer/LLMs/blob/main/chatGLM2_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U accelerate bitsandbytes"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:12:49.662527Z",
          "iopub.execute_input": "2024-01-31T09:12:49.662813Z",
          "iopub.status.idle": "2024-01-31T09:13:02.152702Z",
          "shell.execute_reply.started": "2024-01-31T09:12:49.662789Z",
          "shell.execute_reply": "2024-01-31T09:13:02.151393Z"
        },
        "trusted": true,
        "id": "Vdu3Jnso9h87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:13:02.159035Z",
          "iopub.execute_input": "2024-01-31T09:13:02.15938Z",
          "iopub.status.idle": "2024-01-31T09:13:05.233239Z",
          "shell.execute_reply.started": "2024-01-31T09:13:02.159341Z",
          "shell.execute_reply": "2024-01-31T09:13:05.232412Z"
        },
        "trusted": true,
        "id": "Eqe4UUAo9h89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.__version__"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:13:05.234397Z",
          "iopub.execute_input": "2024-01-31T09:13:05.234786Z",
          "iopub.status.idle": "2024-01-31T09:13:05.241637Z",
          "shell.execute_reply.started": "2024-01-31T09:13:05.234761Z",
          "shell.execute_reply": "2024-01-31T09:13:05.240673Z"
        },
        "trusted": true,
        "id": "1M84UrKE9h8_",
        "outputId": "50a48d4c-b33f-43a7-dd33-ea920f11d828"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'4.36.2'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quanty_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quanty = True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:13:05.244204Z",
          "iopub.execute_input": "2024-01-31T09:13:05.244555Z",
          "iopub.status.idle": "2024-01-31T09:13:05.256246Z",
          "shell.execute_reply.started": "2024-01-31T09:13:05.244523Z",
          "shell.execute_reply": "2024-01-31T09:13:05.255338Z"
        },
        "trusted": true,
        "id": "TpGST3uq9h9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/kaggle/input/chatglm2/pytorch/6b/1\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:41:12.688812Z",
          "iopub.execute_input": "2024-01-31T09:41:12.689474Z",
          "iopub.status.idle": "2024-01-31T09:41:12.693812Z",
          "shell.execute_reply.started": "2024-01-31T09:41:12.689434Z",
          "shell.execute_reply": "2024-01-31T09:41:12.692854Z"
        },
        "trusted": true,
        "id": "84slHQCp9h9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:41:15.030821Z",
          "iopub.execute_input": "2024-01-31T09:41:15.031542Z",
          "iopub.status.idle": "2024-01-31T09:41:15.168418Z",
          "shell.execute_reply.started": "2024-01-31T09:41:15.031512Z",
          "shell.execute_reply": "2024-01-31T09:41:15.167251Z"
        },
        "trusted": true,
        "id": "HgZleD3J9h9D",
        "outputId": "e9d159fd-7412-40c1-be51-c0e8301791f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:784\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 784\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    785\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m         )\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: Tokenizer class ChatGLMTokenizer does not exist or is not currently imported."
          ],
          "ename": "ValueError",
          "evalue": "Tokenizer class ChatGLMTokenizer does not exist or is not currently imported.",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code = True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:41:29.338905Z",
          "iopub.execute_input": "2024-01-31T09:41:29.339288Z",
          "iopub.status.idle": "2024-01-31T09:44:02.742616Z",
          "shell.execute_reply.started": "2024-01-31T09:41:29.339258Z",
          "shell.execute_reply": "2024-01-31T09:44:02.741798Z"
        },
        "trusted": true,
        "id": "TlVH4oll9h9F",
        "outputId": "98f625ac-dcfe-46b1-c624-31c82caea7b0",
        "colab": {
          "referenced_widgets": [
            "0c53f96d708245fead6b657e12d42b16"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c53f96d708245fead6b657e12d42b16"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype = torch.bfloat16,\n",
        "    device_map = \"auto\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:27:29.038081Z",
          "iopub.execute_input": "2024-01-31T09:27:29.038492Z",
          "iopub.status.idle": "2024-01-31T09:27:29.043634Z",
          "shell.execute_reply.started": "2024-01-31T09:27:29.03844Z",
          "shell.execute_reply": "2024-01-31T09:27:29.042661Z"
        },
        "trusted": true,
        "id": "ug5KK2D19h9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"as a data scientist, what is overfitting?\"\n",
        "\n",
        "sequence = pipe(\n",
        "    prompt,\n",
        "    do_sample = True,\n",
        "    max_new_tokens = 100,\n",
        "    temperature = 0.6,\n",
        "    top_k = 150,\n",
        "    top_p = 0.95,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:44:18.996673Z",
          "iopub.execute_input": "2024-01-31T09:44:18.997561Z",
          "iopub.status.idle": "2024-01-31T09:44:26.684555Z",
          "shell.execute_reply.started": "2024-01-31T09:44:18.997529Z",
          "shell.execute_reply": "2024-01-31T09:44:26.683527Z"
        },
        "trusted": true,
        "id": "aBE0-BUO9h9G",
        "outputId": "ca14e2be-8868-497d-9721-496af8b68c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence[0]['generated_text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:44:26.686021Z",
          "iopub.execute_input": "2024-01-31T09:44:26.686289Z",
          "iopub.status.idle": "2024-01-31T09:44:26.691378Z",
          "shell.execute_reply.started": "2024-01-31T09:44:26.686267Z",
          "shell.execute_reply": "2024-01-31T09:44:26.690333Z"
        },
        "trusted": true,
        "id": "AS6r7qKf9h9I",
        "outputId": "d16d35ce-c12f-460d-98e6-c6ea607a6f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "as a data scientist, what is overfitting?\n\nOverfitting is a condition where a model is too complex to be able to generalize well to new data. This usually happens when you have a lot of data, and the model is allowed to fit the data too closely.\n\nIn the case of this data, we have a single sample.\n\n```\n# Create a single sample\nsample = np.arange(1, 100, 1)\n\n# Create a linear regression model\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:46:13.625588Z",
          "iopub.execute_input": "2024-01-31T09:46:13.626395Z",
          "iopub.status.idle": "2024-01-31T09:46:13.634905Z",
          "shell.execute_reply.started": "2024-01-31T09:46:13.626361Z",
          "shell.execute_reply": "2024-01-31T09:46:13.633972Z"
        },
        "trusted": true,
        "id": "FnuwPOLy9h9K",
        "outputId": "40ef0c51-e363-489c-9bf2-e8b54474bd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "ChatGLMForConditionalGeneration(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(in_features=4096, out_features=4608, bias=True)\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(in_features=4096, out_features=27392, bias=False)\n            (dense_4h_to_h): Linear4bit(in_features=13696, out_features=4096, bias=False)\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-31T09:56:44.943448Z",
          "iopub.execute_input": "2024-01-31T09:56:44.944406Z",
          "iopub.status.idle": "2024-01-31T09:56:44.95854Z",
          "shell.execute_reply.started": "2024-01-31T09:56:44.94437Z",
          "shell.execute_reply": "2024-01-31T09:56:44.957622Z"
        },
        "trusted": true,
        "id": "DWkshHG19h9L",
        "outputId": "40cfd667-2123-463b-8a3a-24cd16a623b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "LlamaTokenizerFast(name_or_path='/kaggle/input/mistral/pytorch/7b-v0.1-hf/1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QiBaKb3d9h9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}